{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Denoising_Autoencoder_Exercise.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumanyurosha/deep-learning-v2-pytorch/blob/master/autoencoder/denoising-autoencoder/Denoising_Autoencoder_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNLlHjsmlCLl",
        "colab_type": "text"
      },
      "source": [
        "# Denoising Autoencoder\n",
        "\n",
        "Sticking with the MNIST dataset, let's add noise to our data and see if we can define and train an autoencoder to _de_-noise the images.\n",
        "\n",
        "<img src='https://github.com/sumanyurosha/deep-learning-v2-pytorch/blob/master/autoencoder/denoising-autoencoder/notebook_ims/autoencoder_denoise.png?raw=1' width=70%/>\n",
        "\n",
        "Let's get started by importing our libraries and getting the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH1jyuT0lCLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c379b840-0e82-4ea3-86f2-ff9a75180ed6"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# load the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False,\n",
        "                                  download=True, transform=transform)\n",
        "\n",
        "# Create training and test dataloaders\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8537765.04it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 121276.55it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2129504.10it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 46676.56it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq3yaUDylCL1",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCKXMFKAlCL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "587125de-81ea-4131-8699-c6e63a1b36e5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "    \n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig = plt.figure(figsize = (5,5)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fea0c002a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEvCAYAAAAtufaDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAPl0lEQVR4nO3db4xV9Z3H8c9nUR+IKJB2kVBdKjEY\nNO64QdxYsmpc6p9odNSYTmLDRiM+kASThqzhSfUBhqxKN0RjoBGLpqU2sVY0m1UjKLuxIQ6IirCu\nxqBlMkIUEcR/gfnugzluBjrD+c29d+bOF96vhMy9v/vld7+nRz4959zfPeOIEABk9TftbgAAmkGI\nAUiNEAOQGiEGIDVCDEBqhBiA1E4azTezzXoOAI36NCJ+ePRgU0ditq+2/Z7tD2zf28xcAFDjo8EG\nGw4x2+MkPSrpGkmzJHXZntXofADQiGaOxOZI+iAiPoyI7yT9XtINrWkLAMo0E2LTJP1lwPNd1RgA\njJoRv7Bve4GkBSP9PgBOTM2EWI+kswY8/1E1doSIWCVplcSnkwBar5nTyTcknWv7x7ZPkfQzSeta\n0xYAlGn4SCwiDtleKOlFSeMkrY6Id1vWGQAU8GjeT4zTSQBN2BwRs48e5GtHAFIjxACkRogBSI0Q\nA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIj\nxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDU\nCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUTmp3A8ht3LhxtTVnnHHGKHRypIULFxbV\nnXrqqUV1M2fOLKq7++67a2seeuihorm6urqK6r755pvammXLlhXNdf/99xfVjSVNhZjtnZIOSDos\n6VBEzG5FUwBQqhVHYldExKctmAcAho1rYgBSazbEQtJLtjfbXjBYge0Ftrttdzf5XgDwV5o9nZwb\nET22/1bSy7b/JyI2DiyIiFWSVkmS7Wjy/QDgCE0diUVET/Vzj6RnJc1pRVMAUKrhELM93vaE7x9L\n+qmkba1qDABKNHM6OUXSs7a/n+d3EfGfLekKAAo1HGIR8aGkv29hLxjC2WefXVtzyimnFM116aWX\nFtXNnTu3qG7ixIm1NTfffHPRXGPZrl27iupWrFhRW9PZ2Vk014EDB4rq3nrrrdqa1157rWiujFhi\nASA1QgxAaoQYgNQIMQCpEWIAUiPEAKRGiAFIjRADkBohBiA1R4zejSW4i8WROjo6iurWr19fW9OO\nW0AfD/r6+orqbr/99qK6L7/8spl2jtDb21tU9/nnn9fWvPfee822MxZsHuzu0RyJAUiNEAOQGiEG\nIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUit2d87iSZ8/PHHRXWfffZZbc3xsGJ/06ZNRXX79u2r\nrbniiiuK5vruu++K6p566qmiOow+jsQApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSY7Fr\nG+3du7eobvHixbU11113XdFcb775ZlHdihUriupKbN26tahu3rx5RXUHDx6srTn//POL5lq0aFFR\nHcYujsQApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApOaIGL03s0fvzU4wp59+elHd\ngQMHiupWrlxZVHfHHXfU1tx2221Fc61du7aoDieszREx++jB2iMx26tt77G9bcDYZNsv236/+jmp\n1d0CQImS08nfSLr6qLF7Jb0SEedKeqV6DgCjrjbEImKjpKO/qXyDpDXV4zWSbmxxXwBQpNEL+1Mi\nord6/ImkKS3qBwCGpelb8UREHOuCve0FkhY0+z4AMJhGj8R2254qSdXPPUMVRsSqiJg92KcKANCs\nRkNsnaT51eP5kp5rTTsAMDwlSyzWSvqzpJm2d9m+Q9IySfNsvy/pn6vnADDqaq+JRUTXEC9d2eJe\nAGDYuMf+cWL//v0tne+LL75o2Vx33nlnUd3TTz9dVNfX19dMOzjO8N1JAKkRYgBSI8QApEaIAUiN\nEAOQGiEGIDVCDEBqhBiA1AgxAKlxj30Mavz48UV1zz//fG3NZZddVjTXNddcU1T30ksvFdXhuNPY\nPfYBYCwjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqbHYFU2ZMWNGbc2WLVuK5tq3b19R3YYN\nG2pruru7i+Z69NFHi+pG898JhsRiVwDHH0IMQGqEGIDUCDEAqRFiAFIjxACkRogBSI0QA5AaIQYg\nNVbsY8R1dnYW1T3xxBNFdRMmTGimnSMsWbKkqO7JJ58squvt7W2mHRwbK/YBHH8IMQCpEWIAUiPE\nAKRGiAFIjRADkBohBiA1QgxAaoQYgNRYsY8x44ILLiiqW758eW3NlVde2Ww7R1i5cmVR3dKlS2tr\nenp6mm3nRNXYin3bq23vsb1twNh9tntsb63+XNvqbgGgRMnp5G8kXT3I+K8ioqP68x+tbQsAytSG\nWERslLR3FHoBgGFr5sL+QttvV6ebk4Yqsr3Adrftsl8ECADD0GiIPSZphqQOSb2SHh6qMCJWRcTs\nwS7IAUCzGgqxiNgdEYcjok/SryXNaW1bAFCmoRCzPXXA005J24aqBYCRdFJdge21ki6X9APbuyT9\nUtLltjskhaSdku4awR4BYEgsdkU6EydOrK25/vrri+YqvSW27aK69evX19bMmzevaC78FW5PDeD4\nQ4gBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkxop9nNC+/fbborqTTqr9hp4k6dChQ7U1\nV111VdFcr776alHdCYQV+wCOP4QYgNQIMQCpEWIAUiPEAKRGiAFIjRADkBohBiA1QgxAamXLkIFR\ncOGFFxbV3XLLLbU1F198cdFcpSvxS23fvr22ZuPGjS19zxMdR2IAUiPEAKRGiAFIjRADkBohBiA1\nQgxAaoQYgNQIMQCpEWIAUmPFPpoyc+bM2pqFCxcWzXXTTTcV1Z155plFda10+PDhorre3t7amr6+\nvmbbwQAciQFIjRADkBohBiA1QgxAaoQYgNQIMQCpEWIAUiPEAKTGYtcTTOlC0a6urqK6koWs06dP\nL5qrHbq7u4vqli5dWlS3bt26ZtpBA2qPxGyfZXuD7e2237W9qBqfbPtl2+9XPyeNfLsAcKSS08lD\nkn4REbMk/aOku23PknSvpFci4lxJr1TPAWBU1YZYRPRGxJbq8QFJOyRNk3SDpDVV2RpJN45UkwAw\nlGFd2Lc9XdJFkjZJmhIR33/b9RNJU1raGQAUKL6wb/s0Sc9Iuici9tv+/9ciImzHEH9vgaQFzTYK\nAIMpOhKzfbL6A+y3EfHHani37anV61Ml7Rns70bEqoiYHRGzW9EwAAxU8umkJT0uaUdELB/w0jpJ\n86vH8yU91/r2AODYSk4nfyLp55Lesb21GlsiaZmkP9i+Q9JHkm4dmRYBYGi1IRYR/y3JQ7x8ZWvb\nAYDhYcV+AlOm1H/wO2vWrKK5HnnkkaK68847r6iuHTZt2lRb8+CDDxbN9dxzZVdBuKX02MV3JwGk\nRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkxor9ETB58uSiupUrVxbVdXR01Nacc845\nRXO1w+uvv15U9/DDDxfVvfjii7U1X3/9ddFcyI8jMQCpEWIAUiPEAKRGiAFIjRADkBohBiA1QgxA\naoQYgNRY7Fq55JJLiuoWL15cWzNnzpyiuaZNm1ZU1w5fffVVUd2KFStqax544IGiuQ4ePFhUBwzE\nkRiA1AgxAKkRYgBSI8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1FixX+ns7GxpXStt3769tuaFF14o\nmuvQoUNFdaW3it63b19RHTBSOBIDkBohBiA1QgxAaoQYgNQIMQCpEWIAUiPEAKRGiAFIjRADkJoj\nYvTezB69NwNwvNkcEbOPHqw9ErN9lu0Ntrfbftf2omr8Pts9trdWf64dia4B4FhKvjt5SNIvImKL\n7QmSNtt+uXrtVxHx0Mi1BwDHVhtiEdErqbd6fMD2Dklj93eNATihDOvCvu3pki6StKkaWmj7bdur\nbU9qcW8AUKs4xGyfJukZSfdExH5Jj0maIalD/Udqg967xfYC2922u1vQLwAcoejTSdsnS3pB0osR\nsXyQ16dLeiEiLqiZh08nATSq4U8nLelxSTsGBpjtqQPKOiVta0WXADAcJZ9O/kTSzyW9Y3trNbZE\nUpftDkkhaaeku0akQwA4Bha7AsiisdNJABjLCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNUIMQGqE\nGIDUCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIr+UUhrfSppI+OGvtB\nNZ5V9v6l/NuQvX8p/zaMRv9/N9jgqP6ikEEbsLsHu/l/Ftn7l/JvQ/b+pfzb0M7+OZ0EkBohBiC1\nsRBiq9rdQJOy9y/l34bs/Uv5t6Ft/bf9mhgANGMsHIkBQMPaFmK2r7b9nu0PbN/brj6aYXun7Xds\nb7Xd3e5+SthebXuP7W0Dxibbftn2+9XPSe3s8ViG6P8+2z3Vfthq+9p29ngsts+yvcH2dtvv2l5U\njWfaB0NtQ1v2Q1tOJ22Pk/S/kuZJ2iXpDUldEbF91Jtpgu2dkmZHRJr1Pbb/SdKXkp6MiAuqsX+T\ntDcillX/hzIpIv61nX0OZYj+75P0ZUQ81M7eStieKmlqRGyxPUHSZkk3SvoX5dkHQ23DrWrDfmjX\nkdgcSR9ExIcR8Z2k30u6oU29nFAiYqOkvUcN3yBpTfV4jfr/gxyThug/jYjojYgt1eMDknZImqZc\n+2CobWiLdoXYNEl/GfB8l9r4P0ITQtJLtjfbXtDuZpowJSJ6q8efSJrSzmYatND229Xp5pg9FRvI\n9nRJF0napKT74KhtkNqwH7iw35y5EfEPkq6RdHd1qpNa9F9fyPaR9WOSZkjqkNQr6eH2tlPP9mmS\nnpF0T0TsH/haln0wyDa0ZT+0K8R6JJ014PmPqrFUIqKn+rlH0rPqP03OaHd1neP76x172tzPsETE\n7og4HBF9kn6tMb4fbJ+s/n/8v42IP1bDqfbBYNvQrv3QrhB7Q9K5tn9s+xRJP5O0rk29NMT2+Oqi\npmyPl/RTSduO/bfGrHWS5leP50t6ro29DNv3//grnRrD+8G2JT0uaUdELB/wUpp9MNQ2tGs/tG2x\na/Xx679LGidpdUQsbUsjDbJ9jvqPvqT+u4H8LsM22F4r6XL133Vgt6RfSvqTpD9IOlv9dxm5NSLG\n5MXzIfq/XP2nMCFpp6S7BlxfGlNsz5X0X5LekdRXDS9R/zWlLPtgqG3oUhv2Ayv2AaTGhX0AqRFi\nAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDU/g9v9we25TfpdwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISCNGY8GlCL-",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Denoising\n",
        "\n",
        "As I've mentioned before, autoencoders like the ones you've built so far aren't too useful in practive. However, they can be used to denoise images quite successfully just by training the network on noisy images. We can create the noisy images ourselves by adding Gaussian noise to the training images, then clipping the values to be between 0 and 1.\n",
        "\n",
        ">**We'll use noisy images as input and the original, clean images as targets.** \n",
        "\n",
        "Below is an example of some of the noisy images I generated and the associated, denoised images.\n",
        "\n",
        "<img src='https://github.com/sumanyurosha/deep-learning-v2-pytorch/blob/master/autoencoder/denoising-autoencoder/notebook_ims/denoising.png?raw=1' />\n",
        "\n",
        "\n",
        "Since this is a harder problem for the network, we'll want to use _deeper_ convolutional layers here; layers with more feature maps. You might also consider adding additional layers. I suggest starting with a depth of 32 for the convolutional layers in the encoder, and the same depths going backward through the decoder.\n",
        "\n",
        "#### TODO: Build the network for the denoising autoencoder. Add deeper and/or additional layers compared to the model above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mso9HvFlCMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "94cc6aba-ff9f-4d06-c24e-bebb0decb06d"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the NN architecture\n",
        "class ConvDenoiser(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvDenoiser, self).__init__()\n",
        "        ## encoder layers ##\n",
        "        \n",
        "        # conv layer, depth 1 -> 32\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        # depth 32 -> 16\n",
        "        self.conv2 = nn.Conv2d(32, 16, 3, padding=1)\n",
        "        # depth 16 -> 4\n",
        "        self.conv3 = nn.Conv2d(16, 4, 3, padding=1)\n",
        "        # max pooling layer with kernel 2x2 and stride of 2x2\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        ## decoder layers ##\n",
        "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
        "        self.conv4 = nn.Conv2d(4, 16, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(32, 1, 3, padding=1)\n",
        "\n",
        "        # activation layers\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## encode ##\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        \n",
        "        ## decode ##\n",
        "        x = F.upsample(x, scale_factor=(7/3), mode='nearest')\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = F.upsample(x, scale_factor=2, mode='nearest')\n",
        "        x = self.relu(self.conv5(x))\n",
        "        x = F.upsample(x, scale_factor=2, mode='nearest')\n",
        "        x = self.sigmoid(self.conv6(x))\n",
        "                \n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = ConvDenoiser()\n",
        "print(model)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvDenoiser(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv5): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv6): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQSLWm76lCMK",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Training\n",
        "\n",
        "We are only concerned with the training images, which we can get from the `train_loader`.\n",
        "\n",
        ">In this case, we are actually **adding some noise** to these images and we'll feed these `noisy_imgs` to our model. The model will produce reconstructed images based on the noisy input. But, we want it to produce _normal_ un-noisy images, and so, when we calculate the loss, we will still compare the reconstructed outputs to the original images!\n",
        "\n",
        "Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use `MSELoss`. And compare output images and input images as follows:\n",
        "```\n",
        "loss = criterion(outputs, images)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs5_3nWglCMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# specify loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89SKwl38lCMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "105e2915-3570-4b3e-a4be-e2b5ffa211f9"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 30\n",
        "\n",
        "# for adding noise to images\n",
        "noise_factor=0.5\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    for data in train_loader:\n",
        "        # _ stands in for labels, here\n",
        "        # no need to flatten images\n",
        "        images, _ = data\n",
        "        \n",
        "        ## add random noise to the input images\n",
        "        noisy_imgs = images + noise_factor * torch.randn(*images.shape)\n",
        "        # Clip the images to be between 0 and 1\n",
        "        noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
        "                \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        ## forward pass: compute predicted outputs by passing *noisy* images to the model\n",
        "        outputs = model(noisy_imgs)\n",
        "\n",
        "        # # testing block\n",
        "        # if(True):\n",
        "        #     print(\"output shape: {}\\nimages shape: {}\".format(outputs.shape, images.shape))\n",
        "        #     break\n",
        "        # calculate the loss\n",
        "        # the \"target\" is still the original, not-noisy images\n",
        "        loss = criterion(outputs, images)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "            \n",
        "    # print avg training statistics \n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch, \n",
        "        train_loss\n",
        "        ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 2.247991\n",
            "Epoch: 2 \tTraining Loss: 2.240052\n",
            "Epoch: 3 \tTraining Loss: 2.240052\n",
            "Epoch: 4 \tTraining Loss: 2.240052\n",
            "Epoch: 5 \tTraining Loss: 2.240052\n",
            "Epoch: 6 \tTraining Loss: 2.240052\n",
            "Epoch: 7 \tTraining Loss: 2.240052\n",
            "Epoch: 8 \tTraining Loss: 2.240052\n",
            "Epoch: 9 \tTraining Loss: 2.240052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j8kc2UZlCMb",
        "colab_type": "text"
      },
      "source": [
        "## Checking out the results\n",
        "\n",
        "Here I'm adding noise to the test images and passing them through the autoencoder. It does a suprising great job of removing the noise, even though it's sometimes difficult to tell what the original number is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w1jN1rTlCMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# add noise to the test images\n",
        "noisy_imgs = images + noise_factor * torch.randn(*images.shape)\n",
        "noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
        "\n",
        "# get sample outputs\n",
        "output = model(noisy_imgs)\n",
        "# prep images for display\n",
        "noisy_imgs = noisy_imgs.numpy()\n",
        "\n",
        "# output is resized into a batch of iages\n",
        "output = output.view(batch_size, 1, 28, 28)\n",
        "# use detach when it's an output that requires_grad\n",
        "output = output.detach().numpy()\n",
        "\n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "\n",
        "# input images on top row, reconstructions on bottom\n",
        "for noisy_imgs, row in zip([noisy_imgs, output], axes):\n",
        "    for img, ax in zip(noisy_imgs, row):\n",
        "        ax.imshow(np.squeeze(img), cmap='gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H5XZn0slCMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}